[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mining Acknowledgements Sections",
    "section": "",
    "text": "blah"
  },
  {
    "objectID": "index.html#original-dataset",
    "href": "index.html#original-dataset",
    "title": "Mining Acknowledgements Sections",
    "section": "Original Dataset",
    "text": "Original Dataset\nThe original dataset of 64 papers was provided to us as a large JSON file that had a lot data within it. For our analysis of acknowledgements sections we only needed a few data points to get started. The original dataset is available below for exploration (minor change just to make it render nicely).\n\n\nShow Code for Loading the Original Dataset\nfrom IPython.display import JSON\nimport json\n\nwith open(\"data/599_lit_review.json\", \"r\") as open_f:\n    original_dataset = json.load(open_f)\n    \nJSON({\"data\": original_dataset})\n\n\n<IPython.core.display.JSON object>"
  },
  {
    "objectID": "index.html#compiled-dataset",
    "href": "index.html#compiled-dataset",
    "title": "Mining Acknowledgements Sections",
    "section": "Compiled Dataset",
    "text": "Compiled Dataset\nFor our analysis, we really only needed some metadata and a view or download link for each paper which we could then manually go and copy-paste any acknowledgements section into our dataset (we have some thoughts as to how to automate this in a later section).\nTo extract the data we needed we ran the following code:\n\n\nShow Code for Compile Dataset for Manual Addition\nimport pandas as pd\n\ncompiled_rows = []\nfor index, paper in enumerate(original_dataset):\n    # Some papers have data from CSL and some from S2\n    # Get both so we don't really have to care later on\n    \n    # Check if the paper has CSL data at all\n    if paper.get(\"csl\", None) is not None:\n        # Find or get title and url returned by CSL data\n        csl_title = paper[\"csl\"].get(\"title\", None)\n        csl_url = paper[\"csl\"].get(\"URL\", None)\n    else:\n        csl_title = None\n        csl_url = None\n\n    # Check if the paper has Semantic Scholar data at all\n    if paper.get(\"s2data\", None) is not None:\n        # Find or get title and url returned by S2 data\n        s2_title = paper[\"s2data\"].get(\"title\", None)\n        s2_url = paper[\"s2data\"].get(\"url\", None)\n    else:\n        s2_title = None\n        s2_url = None\n    \n    # Compile all results\n    compiled_rows.append({\n        \"paper_index\": index,\n        \"doi\": paper[\"doi\"],\n        \"s2id\": paper.get(\"s2id\", None),\n        \"s2_url\": s2_url,\n        \"csl_url\": csl_url,\n        \"s2_title\": s2_title,\n        \"csl_title\": csl_title,\n        \"acknowledgements_text\": None,\n    })\n    \ncompiled_dataset = pd.DataFrame(compiled_rows)\ncompiled_dataset.head()\n\n\n\n\n\n\n  \n    \n      \n      paper_index\n      doi\n      s2id\n      s2_url\n      csl_url\n      s2_title\n      csl_title\n      acknowledgements_text\n    \n  \n  \n    \n      0\n      0\n      10.48550/arXiv.2205.02007\n      d1cf6bafac02ac65c7464bc7b168023584a688d7\n      https://www.semanticscholar.org/paper/d1cf6baf...\n      https://arxiv.org/abs/2205.02007\n      A Computational Inflection for Scientific Disc...\n      A Computational Inflection for Scientific Disc...\n      None\n    \n    \n      1\n      1\n      10.23915/distill.00028\n      e291f920147ae876877928e1be94f62ef732e3c6\n      https://www.semanticscholar.org/paper/e291f920...\n      http://dx.doi.org/10.23915/distill.00028\n      Communicating with Interactive Articles\n      Communicating with Interactive Articles\n      None\n    \n    \n      2\n      2\n      10.1111/cgf.13720\n      fe1ea23231e63bdc8738635046e21d7e655e55f2\n      https://www.semanticscholar.org/paper/fe1ea232...\n      http://dx.doi.org/10.1111/cgf.13720\n      Capture & Analysis of Active Reading Behaviors...\n      Capture &amp; Analysis of Active Reading Behav...\n      None\n    \n    \n      3\n      3\n      10.1093/comjnl/27.2.97\n      1ffa1efffa494807ba40ccf6453d156167caa12f\n      https://www.semanticscholar.org/paper/1ffa1eff...\n      http://dx.doi.org/10.1093/comjnl/27.2.97\n      Literate Programming\n      Literate Programming\n      None\n    \n    \n      4\n      4\n      10.1145/3173574.3173606\n      fcc86317d6a0e5f67968d2e84d20dd5ba0524cf2\n      https://www.semanticscholar.org/paper/fcc86317...\n      http://dx.doi.org/10.1145/3173574.3173606\n      Exploration and Explanation in Computational N...\n      Exploration and Explanation in Computational N...\n      None\n    \n  \n\n\n\n\nOur dataset after adding all the acknowledgements sections is available below:\n\n\nRead and Show Data with Acknowledgements Sections Added\nraw_data = pd.read_csv(\"data/raw-ack-sections.csv\")\nraw_data.head()\n\n\n\n\n\n\n  \n    \n      \n      paper_index\n      doi\n      s2id\n      s2_url\n      csl_url\n      s2_title\n      csl_title\n      acknowledgements_text\n    \n  \n  \n    \n      0\n      0\n      10.48550/arXiv.2205.02007\n      d1cf6bafac02ac65c7464bc7b168023584a688d7\n      https://www.semanticscholar.org/paper/d1cf6baf...\n      https://arxiv.org/abs/2205.02007\n      A Computational Inflection for Scientific Disc...\n      A Computational Inflection for Scientific Disc...\n      We thank the members of the Semantic Scholar t...\n    \n    \n      1\n      1\n      10.23915/distill.00028\n      e291f920147ae876877928e1be94f62ef732e3c6\n      https://www.semanticscholar.org/paper/e291f920...\n      http://dx.doi.org/10.23915/distill.00028\n      Communicating with Interactive Articles\n      Communicating with Interactive Articles\n      We are grateful to Arvind Satyanarayan for his...\n    \n    \n      2\n      2\n      10.1111/cgf.13720\n      fe1ea23231e63bdc8738635046e21d7e655e55f2\n      https://www.semanticscholar.org/paper/fe1ea232...\n      http://dx.doi.org/10.1111/cgf.13720\n      Capture & Analysis of Active Reading Behaviors...\n      Capture &amp; Analysis of Active Reading Behav...\n      NaN\n    \n    \n      3\n      3\n      10.1093/comjnl/27.2.97\n      1ffa1efffa494807ba40ccf6453d156167caa12f\n      https://www.semanticscholar.org/paper/1ffa1eff...\n      http://dx.doi.org/10.1093/comjnl/27.2.97\n      Literate Programming\n      Literate Programming\n      The preparation of this paper was supported in...\n    \n    \n      4\n      4\n      10.1145/3173574.3173606\n      fcc86317d6a0e5f67968d2e84d20dd5ba0524cf2\n      https://www.semanticscholar.org/paper/fcc86317...\n      http://dx.doi.org/10.1145/3173574.3173606\n      Exploration and Explanation in Computational N...\n      Exploration and Explanation in Computational N...\n      We thank Regina Cheng and Nathan Hassanzadeh f..."
  },
  {
    "objectID": "index.html#ner",
    "href": "index.html#ner",
    "title": "Mining Acknowledgements Sections",
    "section": "NER",
    "text": "NER\nWe can now take each of these acknowledgements sections and run them through a named entity recognition model.\nLoad the model and show a few examples of what the model produces.\n\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_trf\")\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# Filter dataset to only include rows with acknowledgements sections\nfiltered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n\n# Get random sample\nrandom_example_subset = filtered_data.sample(3)\n\n# Example processed docs\ndocs = []\nfor text in random_example_subset.acknowledgements_text:\n    docs.append(nlp(text))\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n\n\n\ndisplacy.render(docs[0], style=\"ent\")\n\nWe thank \n\n    Regina Cheng\n    PERSON\n\n and \n\n    Nathan Hassanzadeh\n    PERSON\n\n for theirhelp collecting and analyzing data for Studies \n\n    2\n    CARDINAL\n\n and \n\n    3\n    CARDINAL\n\n.  This research   was   funded   by \n\n    NSF\n    ORG\n\n   grants \n\n    #1319829\n    MONEY\n\n and \n\n    #1735234\n    MONEY\n\n as well as \n\n    NLM\n    ORG\n\n grant #T15LM011271.\n\n\n\ndisplacy.render(docs[1], style=\"ent\")\n\n\n\n    A.S.\n    ORG\n\n and \n\n    C.D.\n    ORG\n\n are supported by \n\n    Australian Research Council\n    ORG\n\n grants (DP130100124 and DP190101675). Authors thank \n\n    Jared Hotaling\n    PERSON\n\n and \n\n    Ben R. Newell\n    PERSON\n\n for useful discussion, and various others for their constructive comments on a preprint of the current paper (entitled ‘Preregistration is redundant, at best’).\n\n\n\ndisplacy.render(docs[2], style=\"ent\")\n\nWe would like to thank \n\n    Dastyni Loksa\n    PERSON\n\n for help in designing early versions of the \n\n    VizioMetrics.org\n    ORG\n\n prototype. Thiswork is sponsored in part by \n\n    the National Science Foundation\n    ORG\n\n through \n\n    S2I2\n    ORG\n\n award \n\n    1216879\n    CARDINAL\n\n and \n\n    IIS\n    ORG\n\n awardIII-1064505, a subcontract from \n\n    the Pacific Northwest\nNational Lab\n    ORG\n\n, \n\n    the University of Washington eScience\nInstitute\n    ORG\n\n, \n\n    the Metaknowledge Network\n    ORG\n\n funded by \n\n    the\nJohn Templeton Foundation\n    ORG\n\n, and an award from \n\n    the\nGordon and Betty Moore Foundation\n    ORG\n\n and \n\n    the Alfred\nP. Sloan Foundation\n    ORG\n\n.\n\n\n\n# Filter dataset to only include rows with acknowledgements sections\nfiltered_data = raw_data.dropna(subset=[\"acknowledgements_text\"])\n\n# For each acknowledgement, run it through spacy,\n# extract entities and their labels and store to a dataframe\nentities_rows = []\nfor _, paper in filtered_data.iterrows():\n    doc = nlp(paper.acknowledgements_text)\n    for ent in doc.ents:\n        # Store with the DOI so we can join with other data later\n        entities_rows.append({\n            \"doi\": paper.doi,\n            \"entity\": ent.text,\n            \"entity_label\": ent.label_,\n        })\n        \nentities = pd.DataFrame(entities_rows)\nentities\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n\n\n\n\n\n\n  \n    \n      \n      doi\n      entity\n      entity_label\n    \n  \n  \n    \n      0\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      1\n      10.48550/arXiv.2205.02007\n      2132318\n      CARDINAL\n    \n    \n      2\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      3\n      10.48550/arXiv.2205.02007\n      2040196\n      CARDINAL\n    \n    \n      4\n      10.48550/arXiv.2205.02007\n      ONR\n      ORG\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      398\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      399\n      10.23915/distill.00031\n      the last few years\n      DATE\n    \n    \n      400\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      401\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      402\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n  \n\n403 rows × 3 columns\n\n\n\n\nimport altair as alt\n\nalt.Chart(entities).mark_bar().encode(\n    alt.X(\"entity_label\"),\n    y=\"count()\",\n    color=\"entity_label\",\n    tooltip=\"count()\",\n).properties(\n    width=400,\n    height=300\n).interactive()\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n# Filter all rows that aren't people or orgs\npeople_and_org_refs = entities.loc[entities.entity_label.isin([\"PERSON\", \"ORG\"])]\npeople_and_org_refs\n\n\n\n\n\n  \n    \n      \n      doi\n      entity\n      entity_label\n    \n  \n  \n    \n      0\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      2\n      10.48550/arXiv.2205.02007\n      NSF\n      ORG\n    \n    \n      4\n      10.48550/arXiv.2205.02007\n      ONR\n      ORG\n    \n    \n      6\n      10.23915/distill.00028\n      Arvind Satyanarayan\n      PERSON\n    \n    \n      7\n      10.23915/distill.00028\n      Ludwig Schubert\n      PERSON\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      397\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      398\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      400\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      401\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n    \n      402\n      10.23915/distill.00031\n      Distill\n      ORG\n    \n  \n\n352 rows × 3 columns\n\n\n\n\nalt.Chart(people_and_org_refs).mark_bar().encode(\n    alt.X(\"entity\"),\n    y=\"count()\",\n    color=\"entity\",\n    tooltip=[\"entity\", \"entity_label\", \"count()\"],\n).properties(\n    width=400,\n    height=300\n).interactive()\n\n/usr/share/miniconda/envs/mining-ack/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():"
  },
  {
    "objectID": "index.html#classifying-recognition",
    "href": "index.html#classifying-recognition",
    "title": "Mining Acknowledgements Sections",
    "section": "Classifying Recognition",
    "text": "Classifying Recognition\nblah"
  }
]